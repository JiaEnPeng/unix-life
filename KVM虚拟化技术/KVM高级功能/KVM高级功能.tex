% !TeX spellcheck = en_US
%% 字体：方正静蕾简体
%%		 方正粗宋
\documentclass[a4paper,left=2.5cm,right=2.5cm,11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{fontspec}
\usepackage{cite}
\usepackage{xeCJK}
\usepackage{indentfirst}
\usepackage{titlesec}
\usepackage{longtable}
\usepackage{graphicx}
\usepackage{float}
\usepackage{rotating}
\usepackage{subfigure}
\usepackage{tabu}
\usepackage{amsmath}
\usepackage{setspace}
\usepackage{amsfonts}
\usepackage{appendix}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{geometry}
\setcounter{secnumdepth}{4}
\usepackage{mhchem}
\usepackage{multirow}
\usepackage{extarrows}
\usepackage{hyperref}
\titleformat*{\section}{\LARGE}
\renewcommand\refname{参考文献}
\renewcommand{\abstractname}{\sihao \cjkfzcs 摘{  }要}
%\titleformat{\chapter}{\centering\bfseries\huge\wryh}{}{0.7em}{}{}
%\titleformat{\section}{\LARGE\bf}{\thesection}{1em}{}{}
\titleformat{\subsection}{\Large\bfseries}{\thesubsection}{1em}{}{}
\titleformat{\subsubsection}{\large\bfseries}{\thesubsubsection}{1em}{}{}
\renewcommand{\contentsname}{{\cjkfzcs \centerline{目{  } 录}}}
\setCJKfamilyfont{cjkhwxk}{STXingkai}
\setCJKfamilyfont{cjkfzcs}{STSongti-SC-Regular}
% \setCJKfamilyfont{cjkhwxk}{华文行楷}
% \setCJKfamilyfont{cjkfzcs}{方正粗宋简体}
\newcommand*{\cjkfzcs}{\CJKfamily{cjkfzcs}}
\newcommand*{\cjkhwxk}{\CJKfamily{cjkhwxk}}
\newfontfamily\wryh{Microsoft YaHei}
\newfontfamily\hwzs{STZhongsong}
\newfontfamily\hwst{STSong}
\newfontfamily\hwfs{STFangsong}
\newfontfamily\jljt{MicrosoftYaHei}
\newfontfamily\hwxk{STXingkai}
% \newfontfamily\hwzs{华文中宋}
% \newfontfamily\hwst{华文宋体}
% \newfontfamily\hwfs{华文仿宋}
% \newfontfamily\jljt{方正静蕾简体}
% \newfontfamily\hwxk{华文行楷}
\newcommand{\verylarge}{\fontsize{60pt}{\baselineskip}\selectfont}  
\newcommand{\chuhao}{\fontsize{44.9pt}{\baselineskip}\selectfont}  
\newcommand{\xiaochu}{\fontsize{38.5pt}{\baselineskip}\selectfont}  
\newcommand{\yihao}{\fontsize{27.8pt}{\baselineskip}\selectfont}  
\newcommand{\xiaoyi}{\fontsize{25.7pt}{\baselineskip}\selectfont}  
\newcommand{\erhao}{\fontsize{23.5pt}{\baselineskip}\selectfont}  
\newcommand{\xiaoerhao}{\fontsize{19.3pt}{\baselineskip}\selectfont} 
\newcommand{\sihao}{\fontsize{14pt}{\baselineskip}\selectfont}      % 字号设置  
\newcommand{\xiaosihao}{\fontsize{12pt}{\baselineskip}\selectfont}  % 字号设置  
\newcommand{\wuhao}{\fontsize{10.5pt}{\baselineskip}\selectfont}    % 字号设置  
\newcommand{\xiaowuhao}{\fontsize{9pt}{\baselineskip}\selectfont}   % 字号设置  
\newcommand{\liuhao}{\fontsize{7.875pt}{\baselineskip}\selectfont}  % 字号设置  
\newcommand{\qihao}{\fontsize{5.25pt}{\baselineskip}\selectfont}    % 字号设置 

\usepackage{diagbox}
\usepackage{multirow}
\boldmath
\XeTeXlinebreaklocale "zh"
\XeTeXlinebreakskip = 0pt plus 1pt minus 0.1pt
\definecolor{cred}{rgb}{0.8,0.8,0.8}
\definecolor{cgreen}{rgb}{0,0.3,0}
\definecolor{cpurple}{rgb}{0.5,0,0.35}
\definecolor{cdocblue}{rgb}{0,0,0.3}
\definecolor{cdark}{rgb}{0.95,1.0,1.0}
\lstset{
	language=C,
	numbers=left,
	numberstyle=\tiny\color{white},
	showspaces=false,
	showstringspaces=false,
	basicstyle=\scriptsize,
	keywordstyle=\color{purple},
	commentstyle=\itshape\color{cgreen},
	stringstyle=\color{blue},
	frame=lines,
	% escapeinside=``,
	extendedchars=true, 
	xleftmargin=0em,
	xrightmargin=0em, 
	backgroundcolor=\color{cred},
	aboveskip=1em,
	breaklines=true,
	tabsize=4
} 

\newfontfamily{\consolas}{Consolas}
\newfontfamily{\monaco}{Monaco}
\setmonofont[Mapping={}]{Consolas}	%英文引号之类的正常显示，相当于设置英文字体
\setsansfont{Consolas} %设置英文字体 Monaco, Consolas,  Fantasque Sans Mono
\setmainfont{Times New Roman}

\setCJKmainfont{华文中宋}


\newcommand{\fic}[1]{\begin{figure}[H]
		\center
		\includegraphics[width=0.8\textwidth]{#1}
	\end{figure}}
	
\newcommand{\sizedfic}[2]{\begin{figure}[H]
		\center
		\includegraphics[width=#1\textwidth]{#2}
	\end{figure}}

\newcommand{\codefile}[1]{\lstinputlisting{#1}}

\newcommand{\interval}{\vspace{0.5em}}

\newcommand{\tablestart}{
	\interval
	\begin{longtable}{p{2cm}p{10cm}}
	\hline}
\newcommand{\tableend}{
	\hline
	\end{longtable}
	\interval}

% 改变段间隔
\setlength{\parskip}{0.2em}
\linespread{1.1}

\usepackage{lastpage}
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\space \qquad \space}
\chead{kvm高级功能 \qquad}
\rhead{\qquad\thepage/\pageref{LastPage}}
\begin{document}

\tableofcontents

\clearpage

\section{半虚拟化驱动}
\subsection{QEMU模拟I/O设备的基本原理}
	模拟I/O设备的过程如下：
	\begin{itemize}
		\item[1.] 客户机中的设备驱动程序发起I/O操作请求，KVM模块中的I/O操作捕获代码会拦截这次I/O请求
		\item[2.] I/O操作捕获代码对I/O请求的信息处理后，将其放到I/O共享页，并通知用户控件的QEMU程序
		\item[3.] QEMU模拟程序获得I/O操作的具体信息后，交由硬件模拟代码来模拟出本次的I/O操作
		\item[4.] 硬件模拟代码的模拟操作完成后，把结果放回到I/O共享页，并通知KVM模块的I/O操作捕获代码
		\item[5.] 由KVM模块中的I/O操作捕获代码读取I/O共享页中的操作结果，并把结果返回到客户机中
	\end{itemize}

\subsection{virtio的介绍}
	KVM实现半虚拟化驱动的方式是采用virtio这个Linux上的设备驱动的那个标准框架。\par
	virtio由四层组成，为前端驱动层、virtio层、transport层和后端处理层。
	前端驱动层是客户机中的驱动程序模块，后端处理层是QEMU中的后端处理程序。而virtio层和transport层用于支持客户机和QEMU之间的通信。\par

\subsubsection{virtio\_balloon的介绍}
	首先介绍一下ballooning技术。ballooning技术可以在客户机运行时动态地调整它所占用的宿主机的内存资源，而不需要关闭客户机。
	这个技术实现了，当宿主机内存紧张时，可以请求客户机的部分内存，从而客户机就会释放其空闲内存。
	如果此时客户机空闲内存不足，可能还会回收部分使用中的内存。\par

	KVM中ballooning的工作过程如下：
	\begin{itemize}
		\item[1.] KVM发送请求到客户机操作系统，让其归还部分内存给宿主机。
		\item[2.] 客户机操作系统中的virtio\_balloon驱动接收到KVM的请求，然后使客户机中的内存气球膨胀，气球中的内存不能被客户机访问。
		\item[3.] 客户机操作系统将气球中的内存还给KVM，KVM可以把气球中的内存分配到任何需要的地方。
	\end{itemize}

	使用如下命令即可使用ballooning技术：
	\begin{lstlisting}
	-balloon virtio
	// 如，qemu-system-x86_64 ubuntu1604.img -m 2048 -balloon virtio
	\end{lstlisting}

	可以在qemu monitor中查看和设置客户机内存的大小，命令如下：
	\begin{lstlisting}
	info balloon // 查看客户机内存占用量
	balloon num // 设置客户机内存占用量为numMB
	\end{lstlisting}

	通过如下命令，可以在客户机中看到balloon技术的使用，如下图所示：
	\fic{2.png}

\subsubsection{virtio\_net的介绍}
	选择KVM网络设备时，使用virtio\_net半虚拟化驱动可以提高网络吞吐量和降低网络延迟。\par
	通过以下命令即可将客户机的网卡设备指定为virtio类型：
	\begin{lstlisting}
	-net nic,model=virtio
	// 如，qemu-system-x86_64 ubuntu1604.img -m 2048 -net nic,model=virtio
	\end{lstlisting}

	以下命令可以将virtio\_net的后端处理任务放到内核空间中执行，从而提高效率。如下所示：
	\begin{lstlisting}
	-net tap,vhost=on
	// 如，qemu-system-x86_64 ubuntu1604.img -m 2048 -net nic,model=virtio -net tap,vhost=on
	\end{lstlisting}

\subsubsection{virtio\_blk的介绍}
	使用virtio\_blk半虚拟化驱动可以提高访问块设备I/O的方法。\par
	使用如下命令可以启用virtio\_blk驱动：
	\begin{lstlisting}
	file=filename,if=virtio
	// 如，qemu-system-x86_64 -m 2048 -net nic file=ubuntu1604.img,if=virtio
	\end{lstlisting}

\subsubsection{kvm\_clock的介绍}
	使用kvm\_clock半虚拟化时钟，可以为客户机提供精确的system time和wall time，从而避免客户机时间不准确的问题。\par
	使用qemu命令启动客户机时，已经将kvm\_clock默认作为客户机的时钟来源。可以通过如下命令查看客户机中与时钟相关的信息，如下图所示：
	\fic{1.png}

\section{PCI设备直接分配}
	PCI设备直接分配允许将宿主机中的物理PCI设备直接分配给客户机完全使用。
	Inte定义的PCI设备直接分配技术规范称为VT-d。\par
	当KVM将宿主机的PCI设备附加到客户机时，客户机对该设备的I/O交互操作和实际的物理设备操作完全一样，不需要KVM的参与。

\subsection{VT-d环境配置}
	VT-d环境配置包括以下几个方面：
	\begin{itemize}
		\item[1.] 硬件支持和BIOS设置。需要在BIOS中将VT-d功能设置为“Enabled”状态。
		\item[2.] 宿主机内核的配置。在配置内核时，需要配置如下几个VT-d相关的配置选项：
		\begin{lstlisting}
		CONFIG_IOMMU_SUPPORT=y
		CONFIG_DMAR_TABLE=y
		CONFIG_INTEL_IOMMU=y
		CONFIG_INTEL_IOMMU_DEFAULT_ON=y
		CONFIG_IRQ_REMAP=y
		CONFIG_PCI_STUB=m
		\end{lstlisting}

		可以通过以下两个命令查看宿主机是否支持VT-d：
		\begin{lstlisting}
	dmesg | grep DMAR -i
	dmesg | grep IOMMU -i
		\end{lstlisting}

		\item[3.] 绑定设备到pci\_stub驱动，从而对需要分配给客户机的设备进行隐藏，使得宿主机和其他客户机无法使用该设备。命令如下所示：
		\begin{lstlisting}
	modprobe pci_stub // 加载pci_stub驱动
	// 通过下一行命令得到设备的domain:bus:slot.function vendor_ID:device_ID
	lspci -Dn -s BDF
	// 绑定设备到pci_stub驱动
	echo -n "vendor_ID device_ID" > /sys/bus/pci/drivers/pci-stub/new_id
	echo "domain:bus:slot.function" > /sys/bus/pci/drivers/domain:bus:slot.function/driver/unblind
	echo "domain:bus:slot.function" > /sys/bus/pci/drivers/pci_stub/blind
		\end{lstlisting}

		\item[4.] 使用qemu命令分配设备给客户机，命令如下所示：
		\begin{lstlisting}
	-device pci-assign,host=BDF
	// 如，qemu-system-x86_64 ubuntu1604.img -device pci-assign,host=08:00.0
		\end{lstlisting}

		\item[5.] 当客户机不需要使用该设备后，让宿主机重新使用该设备命令如下：
		\begin{lstlisting}
	echo -n "vendor_ID device_ID" > /sys/bus/pci/drivers/domain:bus:slot.function/driver/new_id
	echo "domain:bus:slot.function" > /sys/bus/pci/drivers/pci_stub/unblind
	echo "domain:bus:slot.function" > /sys/bus/pci/drivers/domain:bus:slot.function/driver/blind
		\end{lstlisting}
	\end{itemize}

	在绑定设备到pci\_stub驱动和使用qemu命令分配设备给客户机两个步骤，主要需要知道设备的BDF。可以通过lspci查看电脑所有设备的BDF，
	每行设备信息前面的bus:slot.function就是设备的BDF。如下图所示：
	\sizedfic{0.75}{3.png}

\subsection{SR-IOV技术}
	SR-IOV技术实现了多个虚拟机能够共享同一个物理设备的资源，并且达到设备直接分配的性能。SR-IOV有两个功能，如下所示：
	\begin{itemize}
		\item[1.] 物理功能(PF)，放在宿主机中配置和管理虚拟功能，它本身也可以作为一个普通的PCI-e设备使用。
		\item[2.] 虚拟功能(VF)，轻量级PCI-e功能。虚拟功能通过物理功能配置后，可以分配到客户机中作为独立功能使用。
	\end{itemize}

	可以通过如下命令查看设备是否具备SR-IOV的能力：
	\begin{lstlisting}
	lspci -v -s BDF
	\end{lstlisting}

	在宿主机中，当加载支持SR-IOV技术的PCI设备的驱动时，可以加上相应的参数来指定启用多少个VF。相关命令如下所示：
	\begin{lstlisting}
	modprobe driver max_vfs=num
	\end{lstlisting}

	在已知设备domain:bus:slot.function的情况下，可以通过以下命令查看该设备的VF：
	\begin{lstlisting}
	ls -l /sys/bus/pci/devices/domain:bus:slot.function/virtfn*
	\end{lstlisting}

\clearpage

\section{热拔插}
	热拔插指的是可以在电脑运行时插上或拔除硬件。在KVM虚拟化环境中，在不关闭客户机的情况下，也可以对客户机的设备进行热拔插。

\subsection{PCI设备的热拔插}
	PCI设备的热拔插需要以下几个方面的支持：
	\begin{itemize}
		\item[1.] 硬件支持。现在的BIOS和PCI总线都支持热拔插。
		\item[2.] 客户机操作系统支持，内核配置文件中需要有以下配置：
		\begin{lstlisting}
		CONFIG_HOTPLUG=y
		CONFIG_HOTPLUG_PCI_PCIE=y
		CONFIG_HOTPLUG_PCI=y
		CONFIG_HOTPLUG_PCI_FAKE=m
		CONFIG_HOTPLUG_PCI_ACPI=y
		CONFIG_HOTPLUG_PCI_ACPI_IBM=m
		\end{lstlisting}
	\end{itemize}

	可以在qemu monitor中完成热拔插功能，比如要将BDF为02:00.0的PCI设备动态添加到客户机中，在monitor中的命令如下：
	\begin{lstlisting}
	device_add pci-assign,host=02:00.0,id=mydevice
	\end{lstlisting}

	也可以将设备从客户机中动态移除，在monitor中的命令如下：
	\begin{lstlisting}
	device_del mydevice
	\end{lstlisting}

	需要注意的是，如果要把宿主机中的PCI设备给客户机作为热拔插使用，需要绑定设备到pci\_stub驱动，从而对需要分配给客户机的设备进行隐藏，使得宿主机和其他客户机无法使用该设备。

\clearpage

\section{动态迁移}
\subsection{虚拟化环境中的迁移}
	在虚拟化环境中的迁移分为静态迁移和动态迁移。\par
	静态迁移有两种的实现方式：
	\begin{itemize}
		\item 一种实现方式是，关闭客户机后，将其硬盘镜像复制到另一台宿主机上然后恢复启动起来。
		\item 另一种实现方式是，两台宿主机共享存储系统，只需要在暂停客户机后，复制其内存镜像到另一台宿主机中恢复启动。
	\end{itemize}

	可以通过以下两个步骤实现静态迁移：
	\begin{itemize}
		\item[1.] 在源宿主机上某客户机的qemu monitor中使用“savevm my\_tag”命令来保存一个完整的客户机镜像快照。
		\item[2.] 在源宿主机中关闭或暂停该客户机。
		\item[3.] 将该客户机的镜像文件复制到另外一台宿主机中，在其qemu monitor中用“loadvm my\_tag”命令来加载保存快照时的客户机快照。
	\end{itemize}

	动态迁移指的是在保证客户机上应用服务正常运行的同时，让客户机在不同的宿主机之间进行迁移。
	一个成功的动态迁移，需要保证客户机的内存、硬盘存储、网络连接在迁移到目的主机后依然保存不变，而且迁移过程的服务暂停时间较短。

\subsection{动态迁移的应用场景}
	\begin{itemize}
		\item[1.] 负载均衡。当一台物理服务器的负载较高时，可以将其上运行的客户机动态迁移到负载较低的宿主机服务器中。
		\item[2.] 解除硬件依赖。当系统管理员需要在宿主机上升级、添加或移除某些硬件设备时，可以将该宿主机上运行的客户机动态迁移到其他宿主机上。
		\item[3.] 节约能源。可以将宿主机上的客户机动态迁移到几台服务器上，而某些宿主机上的客户机完全迁移走后，就可以将其关闭电源，从而省电。
		\item[4.] 实现客户机地理位置上的远程迁移。
	\end{itemize}

\subsection{KVM动态迁移原理}
\subsubsection{基于共享存储系统的动态迁移的原理}
	当源宿主机和目的宿主机共享存储系统时，只需要通过网络发送客户机的vCPU执行状态、内存中的内容和虚拟设备的状态到目的主机上。
	具体迁移过程如下所示：
	\begin{itemize}
		\item[1.] 在客户机在源客户机运行的同时，将客户机的内存页传输到目的主机上。
		\item[2.] KVM会监控并记录下迁移过程中所有已经被传输的内存页的任何修改。
		\item[3.] 当内存数据量传输完成时，KVM会关闭源宿主机上的客户机，然后将剩余的数据量传输到目的主机上去。
		\item[4.] 当所有内存内容传输到目的宿主机后，就可以在目的宿主机上恢复客户机的运行状态。
	\end{itemize}

	需要注意的是，如果目的主机上缺少一些配置，那么客户机就无法正常运行。
	比如，在原宿主机上有给客户机配置好网桥类型的网络，但是目的主机没有网桥配置，那么迁移后的客户机就会网络不通。\par
	还有一种情况就是，如果内存中数据被修改的速度大于KVM能够传输的内存速度时，动态迁移就无法完成。

\subsubsection{动态迁移的注意事项}
	\begin{itemize}
		\item 共享存储在源宿主机和目的宿主机上的挂载位置必须完全一致。
		\item 为了提高动态迁移的成功率，尽量在同类型CPU的主机上面进行动态迁移。
		\item 64位的客户机只能在64位宿主机之间迁移，而32位客户机可以在32位宿主机和64位宿主机之间迁移。
		\item 动态迁移的源宿主机和目的宿主机对NX位的设置必须相同。
		\item 在目的宿主机上不能有与被迁移客户机同名的客户机存在。
		\item 目的宿主机和源宿主机的软件配置应该尽可能相同。
	\end{itemize}

\subsubsection{实现基于共享存储系统的动态迁移}
	动态迁移的实现如下所示：
	\begin{itemize}
		\item[1.] 在源宿主机挂载NFS上的客户机镜像，并启动客户机。命令如下所示：
		\begin{lstlisting}
	// 挂载客户机镜像
	mount my-nfs:/rw-images/ /mnt/
	// 启动客户机
	qemu-system-x86_64 /mnt/ubuntu1604.img -smp 2 -m 2048 -net nic -net tap
		\end{lstlisting}

		\item[2.] 在目的宿主机上挂载NFS上的客户机镜像，并启动一个客户机用于接受动态迁移过来的内存内容。需要注意的是共享存储在源宿主机和目的宿主机上的挂载位置必须完全一致。
		命令如下所示：
		\begin{lstlisting}
	// 挂载客户机镜像
	mount vt-nfs:/rw-images/ /mnt/
	// 启动客户机
	qemu-system-x86_64 /mnt/ubuntu1604.img -smp 2 -m 2048 -net nic -net tap -incoming tcp:0:6666
		\end{lstlisting}

		参数“-incoming tcp:0:6666”表示在6666端口建立一个TCP Socket连接，用于接受来自源主机的动态迁移的内容，其中“0”表示允许来自任何主机的连接。

		\item[3.] 在源宿主机的客户机的qemu monitor中使用如下命令进入动态迁移的流程：
		\begin{lstlisting}
	migrate tcp:vt-snb9:6666
		\end{lstlisting}

		“vt-snb9”是目的宿主机的主机名，tcp协议和6666端口号需要与目的宿主机上qemu-kvm命令行的“-incoming”参数中的值保持一致。
	\end{itemize}

\subsubsection{实现使用相同后端镜像文件的动态迁移}
	过程如下所示：
	\begin{itemize}
		\item[1.] 在源宿主机上，根据一个后端镜像文件，创建一个qcow2格式的镜像文件，并启动客户机。相应的命令如下所示：
		\begin{lstlisting}
	// 创建一个qcow2格式的镜像文件
	qemu-img create -f qcow2 -o backing_file=ubuntu1604.img,size=20G ubuntu1604.qcow2
	// 启动客户机
	qemu-system-x86_64 ubuntu1604.qcow2 -smp 2 -m 2048 -net nic -net tap
		\end{lstlisting}

		\item[2.] 在目的宿主机上，也建立相同的qcow2格式的客户机镜像，并用“-incoming”参数使得客户机处于迁移监听状态。相应的命令如下所示：
		\begin{lstlisting}
	// 创建一个qcow2格式的镜像文件
	qemu-img create -f qcow2 -o backing_file=ubuntu1604.img,size=20G ubuntu1604.qcow2
	// 启动客户机
	qemu-system-x86_64 ubuntu1604.qcow2 -smp 2 -m 2048 -net nic -net tap -incoming tcp:0:6666
		\end{lstlisting}

		\item[3.] 在源宿主机上的客户机的qemu monitor中，运行如下命令进行动态迁移：
		\begin{lstlisting}
	migrate -i tcp:vt-snb9:6666
		\end{lstlisting}
	\end{itemize}

\subsubsection{实现直接复制客户机磁盘镜像的动态迁移}
	过程如下所示：
	\begin{itemize}
		\item[1.] 在源宿主机上启动客户机。相应的命令如下所示：
		\begin{lstlisting}
	// 启动客户机
	qemu-system-x86_64 ubuntu1604.img -smp 2 -m 2048 -net nic -net tap
		\end{lstlisting}

		\item[2.] 在目的宿主机上用“-incoming”参数使得客户机处于迁移监听状态。相应的命令如下所示：
		\begin{lstlisting}
	// 启动客户机
	qemu-system-x86_64 ubuntu1604.img -smp 2 -m 2048 -net nic -net tap -incoming tcp:0:6666
		\end{lstlisting}

		\item[3.] 在源宿主机上的客户机的qemu monitor中，运行如下命令进行动态迁移：
		\begin{lstlisting}
	// -b代表传输块设备
	migrate -b tcp:vt-snb9:6666
		\end{lstlisting}
	\end{itemize}

\subsection{动态迁移的相关命令}
	下面完整介绍一下前几个小节使用的migrate命令：
	\interval
	\begin{longtable}{p{2cm}p{10cm}}
	\hline
	\multicolumn{2}{l}{标准格式：migrate [-d][-b][-i] uri} \\
	\hline
	参数选项： & \\
	\hline
	uri & 统一资源标识符，“协议:主机名:端口号” \\
	\hline
	-d & 表示不用等待迁移完成就让qemu monitor处于可输入命令的状态 \\
	\hline
	-b & 表示传输整个磁盘镜像 \\
	\hline
	-i & 表示在有相同的后端镜像的情况下增量传输qcow2类型的磁盘镜像 \\
	\hline
	\end{longtable}

	\clearpage

	其他命令：
	\interval
	\begin{longtable}{p{7cm}}
	\hline
	migrate\_cancel \\
	在动态迁移进行过程中取消迁移 \\
	\hline
	migrate\_set\_speed value \\
	设置动态迁移中的最大传输速度 \\
	\hline
	migrate\_set\_downtime value \\
	设置允许的最大停机时间 \\
	\hline
	\end{longtable}

\clearpage

\section{嵌套虚拟化}
\subsection{嵌套虚拟化的介绍}
	嵌套虚拟化是指在虚拟化的客户机中运行一个KVM，从而在虚拟化运行一个客户机。\par
	对于KVM这样的必须依靠硬件虚拟化扩展的方案，必须在客户机中模拟硬件虚拟化特性的支持，并在KVM的操作指令进行模拟。

\subsection{KVM嵌套KVM}
	KVM嵌套KVM，就是在KVM上面运行的第一级客户机中，再加载kvm和kvm\_intel模块，然后在第一级客户机中用qemu-kvm启动带有kvm加速的第二级客户机。\par

	KVM嵌套KVM的配置有如下几个步骤：
	\begin{itemize}
		\item[1.] 在宿主机中，加载kvm-intel模块时，需要添加“nested=1”的选项，从而打开嵌套虚拟化的特性。如下所示：
		\begin{lstlisting}
	modprobe kvm_intel nested=1
		\end{lstlisting}

		\item[2.] 在启动第一级客户机时，应该在qemu-kvm命令中加上“-cpu host”或者“-cpu cpu\_model,+vmx”选项，从而将CPU的硬件虚拟化扩展特性暴露给第一级客户机。
		相关的命令如下所示：
		\begin{lstlisting}
	qemu-system-x86_64 ubuntu1604.img -m 4096 -smp 2 -net nic -net tap -cpu host
	// 或者用下面的命令
	qemu-system-x86_64 ubuntu1604.img -m 4096 -smp 2 -net nic -net tap -cpu cpu\_model,+vmx
		\end{lstlisting}

		“-cpu host”参数可以将宿主机的CPU暴露给第一级客户机，而“-cpu cpu\_model,+vmx”参数以某个CPU模型为基础，然后加上Intel VMX特性。

		\item[3.] 在第一级客户机中加载kvm和kvm\_intel模块，然后启动第二级客户机，命令如下：
		\begin{lstlisting}
	modprobe kvm
	modprobe kvm_intel
	qemu-system-x86_64 ubuntu1604.img -m 1024 -smp 2
		\end{lstlisting}
	\end{itemize}

	因为KVM为第一级客户机提供了有硬件辅助虚拟化特性的透明的硬件环境，所以在第一级客户机中启动第二级客户机的操作，和宿主机启动第一级客户机的操作完全一样。

\section{KSM技术}
\subsection{KSM技术的介绍}
	KSM时“Kernel SamePage Merging”的缩写，中文叫做“内核同页合并”。
	KSM让内核扫描正在运行中的程序并比较它们的内存，如果它们的内存页是完全相同的，就将多个相同的内存合并为一个单一的内存页，并将其标识为“写时复制”。
	如果有进程试图去修改被标识为“写时复制”的合并的内存页时，就为该进程复制出一个新的内存页供其使用。\par
	如果同一宿主机上的多个客户机运行的是相同的操作系统，那么客户机之间的相同内存页数量会比较多，于是KSM的作用就比较大。
	在KVM虚拟化环境中，KSM可以从以下两个方面提高内存的速度和使用效率：
	\begin{itemize}
		\item[1.] 相同的内存页合并后，减少了客户机的内存使用量。
		\item[2.] KSM通过减少每个客户机实际占用的内存数量，就可以让多个客户机分配的内存数量之和大于物理上的内存数量。
		这样的话，在物理内存量不变的情况下，可以在一个宿主机中创建更多的客户机，提高了物理资源的利用效率。
	\end{itemize}

\subsection{查看系统中的KSM技术}
	内核的KSM守护进程是ksmd，配置ksmd的文件在“/sys/kernel/mm/ksm/”目录下，可以通过以下命令查看该目录下的几个文件：
	\begin{lstlisting}
	ls -l /sys/kernel/mm/ksm/*
	\end{lstlisting}

	结果如下图所示：
	\sizedfic{0.9}{4.png}

	下面介绍一下这些文件的作用：
	\begin{itemize}
		\item full\_scans: 记录着已经对所有可合并的内存区域扫描过的次数。
		\item pages\_shared: 记录着正在使用中的共享内存页的数量。
		\item pages\_sharing: 记录着有多少数量的内存页正在使用被合并的共享页，不包括合并的内存页本身。
		\item page\_unshared: 记录着守护进程去检查并试图合并，却发现了并没有重复内容而不能被合并的内存页数量。
		\item pages\_volatitle: 记录了因为其内容很容易变化而不被合并的内存页。
		\item pages\_to\_scan: 记录着在ksmd进程休眠之前会去扫描的内存页数量。
		\item sleep\_millisecs: 记录着ksmd进程休眠的时间。
		\item run: 记录着控制ksmd进程是否运行的参数。
	\end{itemize}

	可以通过对pages\_to\_scan、sleep\_millisecs和run这三个文件写入自定义的值来控制ksmd的运行。如下所示：
	\begin{lstlisting}
	// 调整每次扫描的内存页数量
	echo num >/sys/kernel/mm/ksm/pages_to_scan
	// 设置ksmd休眠的时间
	echo num >/sys/kernel/mm/ksm/sleep_millisecs
	// 激活ksmd的运行
	echo 1 >/sys/kernel/mm/ksm/run
	\end{lstlisting}

\clearpage

\section{KVM其他特性简介}
\subsection{1GB大页}
	1GB大页通过hugetlbfs文件系统来利用硬件提供的大页支持。下面是在KVM环境中使用1GB大页的具体操作步骤：
	\begin{itemize}
		\item[1.] 检查硬件和内核配置对1GB大页的支持，相关命令如下所示：
		\begin{lstlisting}
	cat /proc/cpuinfo | grep pdpe1gb
		\end{lstlisting}

		如果没有这个支持，需要在内核编译时配置，config文件需要有以下两项配置：
		\begin{lstlisting}
	CONFIG_HUGETLBFS=y
	CONFIG_HUGETLB_PAGE=y
		\end{lstlisting}

		\item[2.] 在宿主机的内核启动参数重配置1GB，配置文件中添加如下参数：
		\begin{lstlisting}
	hugepagesz=size hugepages=num default_hugepagesz=default_size
	// hugepagesz表示HugeTLB内存页的大小
	// hugepages表示启动时大页分配的数量
	// default_hugepagesz表示在挂在hugetlb文件系统时，没有设置大页的大小时默认使用的大页的大小
	// hugepagesz和hugepages选项可以成对地多次使用，可以让系统在启动时同时保留多个大小不同的大页
		\end{lstlisting}

		如下图所示：
		\fic{5.png}

		\item[3.] 挂在hugetlbfs文件系统，命令行如下：
		\begin{lstlisting}
	mount -t hugetlbfs hugetlbfs /dev/hugepages
		\end{lstlisting}

		如果使用了两种大小的大页，可以在挂在hugetlbfs文件系统的时候，通过“pagesize”选项来制定挂载hugetlbfs的大页的大小，相关命令如下所示：
		\begin{lstlisting}
	mount -t hugetlbfs hugetlbfs /dev/hugepages -o pagesize=size	
		\end{lstlisting}

		\item[4.] 通过“-mem-path”参数为客户机提供大页的支持，命令如下所示：
		\begin{lstlisting}
	qemu-system-x86_64 ubuntu1604.img -mem-path /dev/hugepages/
		\end{lstlisting}
	\end{itemize}

	需要注意的是，如果要使用大页，需要一开始就预留大页的内存。大页的内存不能被交换到交换分区，也不能通过ballooning方式使用大页内存。

\subsection{透明大页}
	透明大页的好处有以下三点：
	\begin{itemize}
		\item[1.] 应用程序不需要任何修改即可享受透明大页带来的好处。
		\item[2.] 透明大页可以被交换到交换空间，此时透明大页会被打碎为常规的4KB大小的内存页。
		\item[3.] 使用透明大页时，如果因为内存碎片导致大页内存分配失败，系统可以优雅地使用常规的4KB页替换。
	\end{itemize}

	使用透明大页的方式如下：
	\begin{itemize}
		\item 在编译Linux内核时，配置好透明大页的支持，如下所示：
		\begin{lstlisting}
	CONFIG_TRANSPARENT_HUGEPAGE=y
	// 表示默认对所有应用程序的内存分配都尽可能地使用透明大页
	CONFIG_TRANSPARENT_HUGEPAGE_ALWAYS=y
		\end{lstlisting}

		除此之外，还可以通过修改内核启动参数来调整透明大页的使用频率。相关参数如下所示：
		\begin{lstlisting}
	transparent_hugepage=[always|madvise|never]
		\end{lstlisting}

		还可以通过以下命令来配置透明大页的使用频率：
		\begin{lstlisting}
	echo "never" > /sys/kernel/mm/transparent_hugepage/defrag
		\end{lstlisting}
	\end{itemize}

\subsection{暴露宿主机CPU特性}
	在启动客户机时，可以用过“+”号来添加一个或多个CPU特性到一个基础的CPU模型上，如下所示：
	\begin{lstlisting}
	-cpu qemu64,+vmx
	\end{lstlisting}

	可以通过如下命令查看宿主机CPU的特性：
	\begin{lstlisting}
	cat /proc/cpuinfo
	\end{lstlisting}

	terminal输出信息中的flags就是宿主机CPU的特性。\par

	qemu-kvm还提供了“-cpu host”参数来尽可能多地暴露宿主机CPU特性给客户机，从而在客户机中可以看到和使用宿主机CPU的各种特性。
	不过“-cpu host”参数可能会阻止客户机的动态迁移，所以需要根据场景使用。

\clearpage

\section{QEMU监控器}
	QEMU监控器是qemu与用户交互的控制台。以下是monitor中一些常用的命令：
	\interval
	\begin{longtable}{p{3cm}p{10cm}}
	\hline
	help [cmd] & 显示命令的帮助信息。 \\
	\hline
	info [subcommand] & 显示subcommand描述的系统状态。如果subcommand为空，就显示当前所有的info命令组合及其介绍。 \\
	\hline
	commit & 将变化部分提交到后端镜像文件中。 \\
	\hline
	stop & 用于停止qemu模拟器 \\
	\hline
	cont & 恢复qemu模拟器继续工作 \\
	\hline
	change & 改变一个设备的配置 \\
	\hline
	balloon & 改变分配给客户机的内存大小 \\
	\hline
	device\_add & 动态添加设备 \\
	\hline
	device\_del & 动态移除设备 \\
	\hline
	usb\_add & 添加一个USB设备 \\
	\hline
	usb\_del & 移除一个USB设备 \\
	\hline
	savevm、loadvm devlvm & 创建、加载和删除客户机的快照 \\
	\hline
	migrate  & 动态迁移和取消动态迁移 \\
	migrate\_cancel & \\
	\hline
	cpu index & 指定系统默认的CPU \\
	\hline
	log & 将制定的项目保存到/tmp/qemu.log中 \\
	\hline
	logfile filename & 将log文件输出到filename文件中 \\
	\hline
	sendkey keys & 像客户机发送keys按键，产生如同在非虚拟环境中那样的按键效果 \\
	\hline
	system\_powerdown & 关闭客户机 \\
	\hline
	system\_reset & 让客户机重置，相当于直接拔掉电源，然后插上电源，按开机键开机 \\
	\hline
	system\_wakeup & 将客户机从暂停中唤醒 \\
	\hline
	x /fmt addr & 转存出从addr开始的虚拟内存地址，fmt指定了转存出来的内存信息的格式 \\
	\hline
	xp /fmt addr & 转存出从addr开始的物理内存地址，fmt指定了转存出来的内存信息的格式 \\
	\hline
	print fmt expr & 按fmt格式打印expr表达式的值 \\
	\hline
	\end{longtable}

	更多的命令可以通过help来查看。

\clearpage

\section{qemu-kvm命令行参数}
\subsection{与配置客户机相关的参数}
	以下是qemu-kvm命令行中一些常用的参数：
	\interval
	\begin{longtable}{p{3cm}p{10cm}}
	\hline
	-help & 查看qemu-kvm命令行中参数以及命令的帮助信息 \\
	\hline
	-cpu参数 & 指定CPU模型，“-cpu ?”可以查询当前qemu-kvm支持哪些CPU模型 \\
	\hline
	-smp参数 & 设置客户机的逻辑CPU \\
	\hline
	-m参数 & 设置客户机内存大小 \\
	\hline
	-mem-path & 为客户机分配内存，主要是分配大页内存 \\
	\hline
	-balloon & 开启内存气球 \\
	\hline
	-hda -hdb -cdrom & 设置客户机的IDE磁盘和光盘设备 \\
	\hline
	-drive & 详细地配置一个驱动器 \\
	\hline
	-boot & 设置客户机启动时的各种选项 \\
	\hline
	-net nic & 为客户机创建一个网卡，并可以详细配置该网卡 \\
	\hline
	-net user & 让客户机使用不需要管理员权限的用户模式网络 \\
	\hline
	-net tap & 使用宿主机的TAP网络接口来帮助客户机建立网络 \\
	\hline
	-net dump & 转存出网络中的数据流量 \\
	\hline
	-net none & 不配置任何的网络设备 \\
	\hline
	-sdl & 使用SDL方式显示客户机 \\
	\hline
	-vnc & 使用VNC方式显示客户机 \\
	\hline
	-vga & 设置客户机中的VGA显卡类型 \\
	\hline
	-nographic & 关闭qemu的图形界面输出 \\
	\hline
	-device & 添加一个设备驱动器 \\
	\hline
	-incoming & 让qemu-kvm进程进入到迁移监听模式，而不是真正以命令行中的镜像文件运行客户机 \\
	\hline
	-daemonize & 让qemu-kvm作为守护进程在后台运行，这样qemu-kvm就不会占用着当前的terminal \\
	\hline
	-usb & 开启客户机中的USB总线 \\
	\hline
	-version & 显示qemu-kvm的版本信息 \\
	\hline
	-k & 设置键盘布局的语言 \\
	\hline
	-soundhw & 开启声卡硬件的支持 \\
	\hline
	-display & 设置显示方式 \\
	\hline
	-name & 设置客户机名称 \\
	\hline
	-uuid & 设置系统的UUID \\
	\hline
	-rtc & 设置RTC开始时间和时钟类型 \\
	\hline
	-chardev & 配置字符型设备 \\
	\hline
	-bios & 指定客户机的BIOS文件 \\
	\hline
	-loadvm & 加载快照状态 \\
	\hline
	-pidfile & 保存进程ID到文件中 \\
	\hline
	-nodefaults & 不创建默认的设备 \\
	\hline
	-readconfig & 从文件中读取客户机设备的配置 \\
	\hline
	-writeconfig & 将客户机中设备的配置写到文件中 \\
	\hline
	-nodefconfig & 使qemu-kvm不加载默认的配置文件 \\
	\hline
	-no-user-config & 使qemu-kvm不加载用户定义的配置文件 \\
	\hline
	\end{longtable}

\subsection{与调试相关的参数}
	qemu-kvm中也有一些与调试相关的参数，如下所示：
	\interval
	\begin{longtable}{p{3cm}p{10cm}}
	\hline
	-singlestep & 以单步执行的模式运行qemu模拟器 \\
	\hline
	-S & 在启动时并不启动CPU，需要在monitor中运行“c”命令才能继续运行 \\
	\hline
	-d & 将qemu的日志保存在/tmp/qemu.log中，以便调试时查看日志 \\
	\hline
	-D logfile & 将qemu的日志保存在logfile中，以便调试时查看日志 \\
	\hline
	\end{longtable}

\clearpage

\section{迁移到KVM虚拟化环境} 
\subsection{从一种虚拟化迁移到另一种虚拟化}
	virt-v2v工具可用于将虚拟客户机从一些hypervisor迁移到KVM环境中。使用如下命令可以安装virt-v2v：
	\begin{lstlisting}
	sudo apt-get install virt-manager
	sudo apt install libguestfs-tools
	\end{lstlisting}

	下面通过一个例子介绍virt-v2v的使用方法。现在要将Xen上的HVM客户机迁移到KVM上，源宿主机的IP地址是192.168.127.163。步骤如下所示：
	\begin{itemize}
		\item[1.] 在源宿主机系统中启动libvirtd服务，并且关闭所要迁移的客户机，该客户机名为xen-hvm1。启动libvirtd服务的命令如下：
		\begin{lstlisting}
		service libvirtd start
		\end{lstlisting}

		\item[2.] 在KVM宿主机中启动libvirtd服务，然后使用virt-v2v工具进行从Xen到KVM的迁移。相关命令如下：
		\begin{lstlisting}
		service libvirtd start
		virt-v2v -ic xen+ssh://root@192.168.127.163 -o default -b br0 xen-hvm1
		\end{lstlisting}

		从Xen上迁移过来的客户机，其镜像文件默认在/var/lib/libvirt/images/目录下，其XML配置文件默认在/etc/libvirt/qemu/目录下。

		\item[3.] 根据客户机的配置文件直接开启客户机，命令如下：
		\begin{lstlisting}
		virsh create /etc/libvirt/qemu/xen-hvm1.xml
		\end{lstlisting}
	\end{itemize}

	这里介绍一下virt-v2v工具：
	\interval
	\begin{longtable}{p{3cm}p{10cm}}
	\hline
	\multicolumn{2}{l}{从Xen客户机迁移到KVM的命令如下：} \\
	\multicolumn{2}{l}{virt-v2v -ic xen+ssh://root@xen0.demo.com -os pool -b brname vm-name} \\
	\hline
	\multicolumn{2}{l}{其中的参数介绍如下：} \\
	\hline
	-ic URI & 表示连接远程Xen宿主机中libvirt的URI \\
	\hline
	-os pool & 表示迁移过来后，用于存放镜像文件的本地目录 \\
	\hline
	-b brname & 表示本地网桥的名称，用于建立与客户机的网络连接。如果本地使用虚拟网络，则使用-n nerwork参数来指定虚拟网络 \\
	\hline
	vm-name & 表示在Xen的源宿主机中将要被迁移的客户机的名称 \\
	\hline
	\end{longtable}

\subsection{从VMware迁移到KVM}
\subsubsection{借助virt-v2v工具实现迁移}
	从VMware迁移到KVM的方法，与前一节中从Xen迁移到KVM的完全类似。
	下面通过一个例子介绍迁移过程。现在要将VMware ESX上的一个客户机迁移到KVM上，命令行如下：
	\begin{lstlisting}
	virt-v2v -ic esx+ssh://esx.demo.com/ -os pool -b brname vm-name
	\end{lstlisting}

	当连接到VMware的ESX服务器时，一般需要认证和授权，此时virt-v2v读取\$HOME/.netrc文件中的机器名、用户名和密码用于认证。\$HOME/.netrc文件中信息的格式如下所示：
	\begin{lstlisting}
	machine esx.demo.com login root password 123456
	\end{lstlisting}

\subsubsection{通过复制客户机镜像实现迁移}
	这个方法很直接：先关闭VMware客户机，然后直接将VMware的客户机镜像复制到KVM的客户机镜像存储系统上，接着通过qemu-kvm命令行工具启动该镜像文件即可。
	命令如下所示：
	\begin{lstlisting}
	qemu-system-x86_64 -m 1024 win7.vmdk
	\end{lstlisting}

	也可以将vmdk的镜像文件转化为qcow2格式，然后用qemu-kvm命令行启动。相关命令如下所示：
	\begin{lstlisting}
	qemu-img convert win7.vmdk -O qcow2 win7.qcow2
	qemu-system-x86_64 -m 1024 win7.qcow2
	\end{lstlisting}

\subsection{从VirtualBox迁移到KVM}
	从VirtualBox迁移到KVM完全类似于从VMware迁移到KVM。\par
	借助virt2-v2v的方法如下所示：
	\begin{lstlisting}
	virt-v2v -ic vbox+ssh://root@vbox.demo.com -os pool -b brname vm-name
	\end{lstlisting}

	也可以直接将VirtualBox中的客户机镜像文件复制到KVM宿主机中使用，qemu-kvm可以直接将.vdi格式的镜像文件作为客户机镜像直接启动：
	\begin{lstlisting}
	qemu-system-x86_64 -m 1024 ubuntu.vdi
	\end{lstlisting}

	也可以将.vdi的镜像文件转化为qcow2格式，然后用qemu-kvm命令行启动。相关命令如下所示：
	\begin{lstlisting}
	qemu-img convert ubuntu.vdi -O qcow2 ubuntu.qcow2
	qemu-system-x86_64 -m 1024 ubuntu.qcow2
	\end{lstlisting}

\subsection{从物理机迁移到KVM虚拟化环境}
	使用virt-p2v工具的方法将物理机迁移到KVM虚拟化环境中需要经过以下几个步骤：
	\begin{itemize}
		\item[1.] 在KVM宿主机上安装virt-v2v、libvirt等工具。
		\item[2.] 修改宿主机/etc/virt-v2v.conf配置文件，如下所示：\par
		首先打开/etc/virt-v2v.conf文件，命令如下：
		\begin{lstlisting}
	sudo vi /etc/virt-v2v.conf
		\end{lstlisting}

		然后将其配置为如下内容：
		\fic{6.png}

		\item[3.] 制作virt-p2v可以启动的设备。将virt-p2vISO镜像文件烧录到U盘中，作为物理机的启动设备。
		\item[4.] 在待迁移的物理机上，选择第3步中制作的设备来启动系统。
		\item[5.] 在virt-p2v客户端启动后，根据提示，填写第二步中所写的服务端的IP、登陆用户等信息。
		在连接到服务器端后，可以进一步填写转移后的虚拟客户机的配置，最后点击“convert”按钮，
		virt-p2v客户端会讲物理机中的磁盘信息通过网络传输到服务器端，待传输完成后，选择关闭该物理机即可。
		\item[6.] 在KVM宿主机上通过qemu-kvm命令行启动第5步转移过来的客户机即可。
	\end{itemize}

\end{document}
